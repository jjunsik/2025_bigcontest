"""
ë¬¸ì„œ ì²­í‚¹ ë° ë²¡í„°DB ì ì¬ ì„œë¹„ìŠ¤
- ê° rowë¥¼ ê°œë³„ ë¬¸ì„œë¡œ ì €ì¥
- contentë§Œ ì„ë² ë”© (ìˆœìˆ˜ ë‚´ìš©)
- metadataëŠ” Documentì— ì €ì¥ (ê²€ìƒ‰ ê²°ê³¼ì™€ í•¨ê»˜ ë°˜í™˜)
"""

import os
import shutil
import time

import pandas as pd
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain_community.vectorstores.utils import DistanceStrategy

from rag.vectorstore.embeddings import get_embeddings

FAISS_PATH = "./faiss_db"
BATCH_SIZE = 7
DELAY_SECONDS = 10


def ingest_youtube_tips_csv(csv_path: str, append_mode: bool = True) -> int:
    """
    ìœ íŠœë¸Œ íŒ CSVë¥¼ ë²¡í„°DBì— ì ì¬

    ì €ì¥ ë°©ì‹:
    - page_content: contentë§Œ (ì„ë² ë”© ëŒ€ìƒ)
    - metadata: channel, title, video_link (ê²€ìƒ‰ ê²°ê³¼ì™€ í•¨ê»˜ ë°˜í™˜)

    Args:
        csv_path: CSV íŒŒì¼ ê²½ë¡œ
        append_mode: Trueë©´ ê¸°ì¡´ ë°ì´í„°ì— ì¶”ê°€, Falseë©´ ì™„ì „ êµì²´

    Returns:
        ì ì¬ëœ ë¬¸ì„œ ê°œìˆ˜
    """
    df = pd.read_csv(csv_path)
    print(f"ğŸ“Š CSV íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ í–‰")

    # ============================================
    # 1. video_linkë³„ metadata ìˆ˜ì§‘
    # ============================================
    video_metadata = {}

    for idx, row in df.iterrows():
        if pd.isna(row['video_link']) or str(row['video_link']).strip() == '':
            continue

        video_link = str(row['video_link']).strip()

        if video_link not in video_metadata:
            video_metadata[video_link] = {
                "channel": None,
                "title": None
            }

        if video_metadata[video_link]["channel"] is None and pd.notna(row.get('channel')):
            video_metadata[video_link]["channel"] = str(row['channel'])

        if video_metadata[video_link]["title"] is None and pd.notna(row.get('title')):
            video_metadata[video_link]["title"] = str(row['title'])

    # metadata ê¸°ë³¸ê°’ ì„¤ì •
    for video_link, metadata in video_metadata.items():
        if metadata["channel"] is None:
            metadata["channel"] = "ì•Œ ìˆ˜ ì—†ìŒ"
        if metadata["title"] is None:
            metadata["title"] = "ì œëª© ì—†ìŒ"

    # ============================================
    # 2. ê° rowë¥¼ ê°œë³„ ë¬¸ì„œë¡œ ìƒì„±
    # ============================================
    texts = []
    metadatas = []
    skipped_rows = 0

    for idx, row in df.iterrows():
        if pd.isna(row['video_link']) or str(row['video_link']).strip() == '':
            skipped_rows += 1
            continue

        video_link = str(row['video_link']).strip()
        channel = video_metadata[video_link]["channel"]
        title = video_metadata[video_link]["title"]

        # content ìˆ˜ì§‘
        content_parts = []

        if pd.notna(row.get('content_marketing')):
            content_parts.append(f"[ë§ˆì¼€íŒ… ì „ëµ]\n{row['content_marketing']}")

        if pd.notna(row.get('content_issue')):
            content_parts.append(f"[ë¬¸ì œ ìƒí™©]\n{row['content_issue']}")

        if pd.notna(row.get('content_solution')):
            content_parts.append(f"[í•´ê²° ë°©ë²•]\n{row['content_solution']}")

        if not content_parts:
            skipped_rows += 1
            continue

        # ============================================
        # í•µì‹¬: contentë§Œ page_contentë¡œ ì‚¬ìš© (ì„ë² ë”© ëŒ€ìƒ)
        # ============================================
        combined_content = "\n\n".join(content_parts)

        # page_content: contentë§Œ (metadata ì œì™¸)
        text = combined_content

        # metadata: Documentì— ì €ì¥ (ê²€ìƒ‰ ê²°ê³¼ì™€ í•¨ê»˜ ë°˜í™˜)
        metadata = {
            "channel": channel,
            "title": title,
            "video_link": video_link
        }

        texts.append(text.strip())
        metadatas.append(metadata)

    if skipped_rows > 0:
        print(f"âš ï¸ ì´ {skipped_rows}ê°œ í–‰ ê±´ë„ˆëœ€")

    # ============================================
    # 3. ì ì¬
    # ============================================
    total_docs = len(texts)

    if total_docs == 0:
        print("âŒ ì ì¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0

    print(f"ğŸ“ ì´ {total_docs}ê°œ ë¬¸ì„œ ìƒì„± ì™„ë£Œ")
    print(f"   ì„ë² ë”© ëŒ€ìƒ: contentë§Œ (ìˆœìˆ˜ ë‚´ìš©)")
    print(f"   metadata ì €ì¥: channel, title, video_link")
    print(f"â±ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ {(total_docs // BATCH_SIZE) * DELAY_SECONDS // 60 + 1}ë¶„")

    embeddings = get_embeddings(task_type="retrieval_document")

    # ============================================
    # 4. ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    # ============================================
    vectorstore = None

    if append_mode and os.path.exists(FAISS_PATH):
        try:
            print(f"ğŸ“‚ ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")
            vectorstore = FAISS.load_local(
                FAISS_PATH,
                embeddings,
                allow_dangerous_deserialization=True
            )
            print(f"âœ… ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            vectorstore = None

    # ============================================
    # 5. ë°°ì¹˜ ì²˜ë¦¬
    # ============================================
    for i in range(0, total_docs, BATCH_SIZE):
        batch_texts = texts[i:i+BATCH_SIZE]
        batch_metadatas = metadatas[i:i+BATCH_SIZE]

        batch_num = i // BATCH_SIZE + 1
        total_batches = (total_docs + BATCH_SIZE - 1) // BATCH_SIZE
        print(f"\në°°ì¹˜ {batch_num}/{total_batches}: {len(batch_texts)}ê°œ ë¬¸ì„œ")

        for j, metadata in enumerate(batch_metadatas):
            print(f"  - ë¬¸ì„œ {i+j+1}: {metadata['channel']} | {metadata['title'][:30]}...")

        batch_documents = [
            Document(page_content=text, metadata=metadata)
            for text, metadata in zip(batch_texts, batch_metadatas)
        ]

        try:
            if vectorstore is None:
                vectorstore = FAISS.from_documents(
                    batch_documents,
                    embeddings,
                    distance_strategy=DistanceStrategy.COSINE
                )
            else:
                vectorstore.add_documents(batch_documents)

            print(f"  âœ… ë°°ì¹˜ {batch_num} ì™„ë£Œ")
        except Exception as e:
            print(f"  âŒ ë°°ì¹˜ {batch_num} ì‹¤íŒ¨: {e}")
            print(f"  â³ 20ì´ˆ ëŒ€ê¸°...")
            time.sleep(20)
            continue

        if i + BATCH_SIZE < total_docs:
            print(f"  â³ {DELAY_SECONDS}ì´ˆ ëŒ€ê¸°...")
            time.sleep(DELAY_SECONDS)

    # ============================================
    # 6. ì €ì¥
    # ============================================
    if vectorstore:
        vectorstore.save_local(FAISS_PATH)
        print(f"\nâœ… ì´ {total_docs}ê°œ ë¬¸ì„œ ì ì¬ ì™„ë£Œ!")

        try:
            final_vectorstore = FAISS.load_local(
                FAISS_PATH,
                embeddings,
                allow_dangerous_deserialization=True
            )
            total_count = final_vectorstore.index.ntotal
            print(f"ğŸ“Š ë²¡í„°ìŠ¤í† ì–´ ìµœì¢… ë¬¸ì„œ ê°œìˆ˜: {total_count}ê°œ")
        except:
            pass

        return total_docs
    else:
        print(f"âŒ ì ì¬ ì‹¤íŒ¨")
        return 0


def clear_vectorstore():
    """ë²¡í„°DB ì´ˆê¸°í™”"""
    if os.path.exists(FAISS_PATH):
        shutil.rmtree(FAISS_PATH)
        print(f"ğŸ—‘ï¸ ë²¡í„°ìŠ¤í† ì–´ ì‚­ì œ ì™„ë£Œ")
        return True
    else:
        print(f"âš ï¸ ì‚­ì œí•  ë²¡í„°ìŠ¤í† ì–´ ì—†ìŒ")
        return False
